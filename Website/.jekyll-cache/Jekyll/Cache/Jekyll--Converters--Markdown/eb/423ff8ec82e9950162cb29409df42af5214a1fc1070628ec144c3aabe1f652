I"<h2 id="problem-1-20-points">Problem 1 [20 Points]</h2>

<h3 id="subproblem-1-10-points">Subproblem 1 [10 Points]</h3>

<p>A train bridge is constructed across a wide river. Trains arrive at the bridge according to a Poisson
process of rate \(\lambda\) = 3 per day.</p>
<ul>
  <li>Find the probability that no trains arrive in the first two days, but 4 trains arrive on the 4th day.</li>
  <li>Find the probability that it takes 2 days for the 5th train to arrive at the bridge.</li>
</ul>

<h3 id="subproblem-2-10-points">Subproblem 2 [10 Points]</h3>

<p>Let \(X\) and \(Y\) have joint PDF 
\(f_{X,Y}(x,y) = \begin{cases}
C e^{-(ax+by)} &amp; x,y \geq 0\\
0 &amp; \text{otherwise}
\end{cases},\)</p>

<p>where, \(a, b &gt; 0\) are constants.</p>

<ul>
  <li>Determine the constant \(C\)</li>
  <li>Find the marginal density of \(X\) and \(Y\). What can you infer from this?</li>
  <li>Find \(\mathbb{E}(Y \mid X&gt; \frac{\exp(a + b)}{a^2 + b^2})\)</li>
  <li>Find \(\mathbb{P}(0 &lt; X &lt; \sqrt{Y})\)</li>
</ul>

<h2 id="problem-2-20-points">Problem 2 [20 points]</h2>

<p>Consider a \(\{0/1\}\) problem, where you have totally \(n_B\) observations and each them should take value from \(\{0, 1\}\) (like throwing a coin). The number of positive sides \(y_B\) (that you get \(1\) in the trial) could be represented by Binomial distribution \(p(y_B\lvert\theta_B)\) where \(\theta_B\) is the Bernoulli probability. You can consider a Beta distribution as the prior of the \(\theta_B\).</p>
<ul>
  <li>Make some comments on why Beta distribution may be a good choice for this problem.</li>
  <li>Write down the expression for the conditional distribution of \(p(y_B\lvert\theta_B)\) and the prior \(p(\theta_B)\).</li>
  <li>Derive the posterior distribution of \(\theta_B\).</li>
  <li>Consider a special case: \(n_B = 40\) an \(y_B = 0\). What is your estimation of \(\theta_B\) if you are about to use maximum likelihood estimation? What is the confidence interval of that estimation? Do you see the problem?</li>
</ul>

<h2 id="problem-3-20-points">Problem 3 [20 Points]</h2>

<h3 id="subproblem-1-10-points-1">Subproblem 1 [10 Points]</h3>

<p>Consider a mixture of three univariate Gaussian distributions:</p>

\[p(x) = \alpha p_1(x) + \beta p_2(x) + \gamma p_3(x),\]

<p>where \(\alpha, \beta, \gamma \in [0,1]\) are mixture weights satisfying \(\alpha + \beta + \gamma = 1\) and \(p_1(x)\), \(p_2(x)\) and \(p_3(x)\) are univariate Gaussians with different parameters \((\mu_1, \sigma_1) \neq (\mu_2, \sigma_2) \neq (\mu_3, \sigma_3)\).</p>

<p>Derive the expectation and variance of \(p(x)\), analytically, using their definitions.</p>

<h3 id="subproblem-2-10-points-1">Subproblem 2 [10 Points]</h3>

<p>For \((\mu_1, \sigma_1) = (-1, 0.4)\), \((\mu_2, \sigma_2) = (1, 0.3)\), and \((\mu_3, \sigma_3) = (0, 0.5)\) write a python script that employs the <a href="https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.random.multivariate_normal.html">univariate Gaussian</a> function to generate \(1000\) samples from \(p(x)\) for \(\alpha = [0.1, 0.3, 0.5, 0.7], \beta = [0.2, 0.5, 0.1, 0]\) and plot the results in different figures. Make a qualitative comment on the form of the results as \(\alpha, \beta\) and \(\gamma\) change.</p>

<h2 id="problem-4-20-points">Problem 4 [20 Points]</h2>

<p>Consider two multivariate normal distributions \(p(x) \sim \mathcal{N}(\mathbf{x}; \mathbf{\mu}_1, \mathbf{\Sigma}_1)\) and \(q(x) \sim \mathcal{N}(\mathbf{x}; \mathbf{\mu}_2, \mathbf{\Sigma}_2)\), with \(\mathcal{N}(\mathbf{x}; \mathbf{\mu}, \mathbf{\Sigma})\) defined as</p>

<p>\begin{align}
    \mathcal{N}(\mathbf{x}; \mathbf{\mu}, \mathbf{\Sigma}) = \frac{1}{\sqrt{(2 \pi)^n \lvert\mathbf{\Sigma}\lvert}} \exp \left[ -\frac{1}{2} (\mathbf{x} - \mathbf{\mu})^\top \Sigma^{-1} (\mathbf{x} - \mathbf{\mu}) \right]
\end{align}</p>

<ul>
  <li>Analytically find the KL divergence \(\mathbb{KL}(P \lvert\lvert Q)\).</li>
  <li>Assume \(\mu_1 = [0, 1], \mu_2 = [2, 1], \sigma_1 = [[1, 2];[2, 0.6]], \sigma_2 = [[0.7, 3];[3, 1.5]]\). Sample 1,000 data from each distribution and use kernel density estimation to fit a probability density to the generated samples. Compute the KL-divergence numerically and compare it with the result you obtained from question (1). If you increase / reduce the number of samples, what do you observe?</li>
</ul>

<h2 id="problem-5-20-points">Problem 5 [20 Points]</h2>

<p>Consider a two-dimensional random variable \(z=(z_1,z_2)\) distributed as,</p>

<p>\begin{align}
p(z_1,z_2) = \mathcal{N}\left(\left[\begin{matrix} 0 \ 0 \end{matrix}\right],
\left[\begin{matrix} 1 &amp; 0 \ 0 &amp; 1 \end{matrix}\right]\right).
\end{align}</p>

<p>Also consider a transformation \(z_1 = g_1(x_1) = x_1^3 - 3x_1^2 + 3x_1 - 1\) whose inverse mapping is \(x_1 = g_1^{-1}(z_1) = z_1^{1/3} + 1\), and \(z_2 = g_2(x_2) = x_2 + 0.5\) whose inverse mapping is \(x_2 = g_2^{-1}(z_2) = z_2 - 0.5\). Generate \(2000\) realizations for \((z_1, z_2)\) from \(p(z_1,z_2)\) and compute the corresponding \((x_1, x_2)\) pairs according to the given transformation. Then, maximize the log-likelihood of the \((x_1, x_2)\) observations to estimate the parameters of the forward mapping that transforms \(p(z)\) into \(p(x)\). Report the learned parameters, log-likelihood loss and a visualization of \(p(x)\) at every step of your gradient ascent algorithm.</p>
:ET