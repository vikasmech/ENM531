---
layout: assignment
categories:
 - assignment
title: "Assignment 1:  A primer on Probability and Statistics (due on 02/16)"
due:
tags:
 - week-2
permalink: /assn1/
---

## Problem 1 [20 Points]

### Subproblem 1 [10 Points]

A train bridge is constructed across a wide river. Trains arrive at the bridge according to a Poisson
process of rate $$\lambda$$ = 3 per day.
- Find the probability that no trains arrive in the first two days, but 4 trains arrive on the 4th day.
- Find the probability that it takes 2 days for the 5th train to arrive at the bridge.

### Subproblem 2 [10 Points]

Let $$X$$ and $$Y$$ have joint PDF 
$$f_{X,Y}(x,y) = \begin{cases}
C e^{-(ax+by)} & x,y \geq 0\\
0 & \text{otherwise}
\end{cases},$$

where, $$a, b > 0$$ are constants

- Determine the constant $$C$$
- Find the marginal density of $$X$$ and $$Y$$. What can you infer from this?
- Find $$\mathbb{E}(Y \mid X> \frac{\exp(a + b)}{a^2 + b^2})$$
- Find $$\mathbb{P}(0 < X < \sqrt{Y})$$

## Problem 2 [20 points]

Consider a $$\{0/1\}$$ problem, where you have totally $$n_B$$ observations and each them should take value from $$\{0, 1\}$$ (like throwing a coin). The number of positive sides $$y_B$$ (that you get $$1$$ in the trial) could be represented by Binomial distribution $$p(y_B|\theta_B)$$ where $$\theta_B$$ is the Bernoulli probability. You can consider a Beta distribution as the prior of the $$\theta_B$$.
- Make some comments on why Beta distribution may be a good choice for this problem.
- Write down the expression for the conditional distribution of $$p(y_B|\theta_B)$$ and the prior $$p(\theta_B)$$.
- Derive the posterior distribution of $$\theta_B$$.
- Consider a special case: $$n_B = 40$$ an $$y_B = 0$$. What is your estimation of $$\theta_B$$ if you are about to use maximum likelihood estimation? What is the confidence interval of that estimation? Do you see the problem?

## Problem 3 [20 Points]

### Subproblem 1 [10 Points]

Consider a mixture of three univariate Gaussian distributions:
$$p(x) = \alpha p_1(x) + \beta p_2(x) + \gamma p_3(x)$$

where $$\alpha, \beta, \gamma \in [0,1]$$ are mixture weights satisfying $$\alpha + \beta + \gamma = 1$$ and $$p_1(x)$$, $$p_2(x)$$ and $$p_3(x)$$ are univariate Gaussians with different parameters $$(\mu_1, \sigma_1) \neq (\mu_2, \sigma_2) \neq (\mu_3, \sigma_3)$$.

Derive the expectation and variance of $$p(x)$$, analytically, using their definitions.

### Subproblem 2 [10 Points]

For $$(\mu_1, \sigma_1) = (-1, 0.4)$$, $$(\mu_2, \sigma_2) = (1, 0.3)$$, and $$(\mu_3, \sigma_3) = (0, 0.5)$$ write a python script that employs the [univariate Gaussian](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.random.multivariate_normal.html) function to generate $$1000$$ samples from $$p(x)$$ for $$\alpha = [0.1, 0.3, 0.5, 0.7], \beta = [0.2, 0.5, 0.1, 0]$$ and plot the results in different figures. Make a qualitative comment on the form of the results as $$\alpha, \beta$$ and $$\gamma$$ change.

## Problem 4 [20 Points]

Consider two multivariate normal distributions $$p(x) \sim \mathcal{N}(\mathbf{x}; \mathbf{\mu}_1, \mathbf{\Sigma}_1)$$ and $$q(x) \sim \mathcal{N}(\mathbf{x}; \mathbf{\mu}_2, \mathbf{\Sigma}_2)$$, with $$\mathcal{N}(\mathbf{x}; \mathbf{\mu}, \mathbf{\Sigma})$$ defined as

\begin{align}
    \mathcal{N}(\mathbf{x}; \mathbf{\mu}, \mathbf{\Sigma}) = \frac{1}{\sqrt{(2 \pi)^n |\mathbf{\Sigma}|}} \exp \left[ -\frac{1}{2} (\mathbf{x} - \mathbf{\mu})^\top \Sigma^{-1} (\mathbf{x} - \mathbf{\mu}) \right]
\end{align}

- Analytically find the KL divergence $$\mathbb{KL}(P || Q)$$
- Assume $$\mu_1 = [0, 1], \mu_2 = [2, 1], \sigma_1 = [[1, 2];[2, 0.6]], \sigma_2 = [[0.7, 3];[3, 1.5]]$$. Sample 1,000 data from each distribution and use kernel density estimation to fit a probability density to the generated samples. Compute the KL-divergence numerically and compare it with the result you obtained from question (1). If you increase / reduce the number of samples, what do you observe?

## Problem 5 [20 Points]

Consider a two-dimensional random variable $$z=(z_1,z_2)$$ distributed as

\begin{align}
p(z_1,z_2) = \mathcal{N}\left(\left[\begin{matrix} 0 \\ 0 \end{matrix}\right],
\left[\begin{matrix} 1 & 0 \\ 0 & 1 \end{matrix}\right]\right).
\end{align}


Also consider a transformation $$z_1 = g_1(x_1) = x_1^3 - 3x_1^2 + 3x_1 - 1$$ whose inverse mapping is $$x_1 = g_1^{-1}(z_1) = z_1^{1/3} + 1$$, and $$z_2 = g_2(x_2) = x_2 + 0.5$$ whose inverse mapping is $$x_2 = g_2^{-1}(z_2) = z_2 - 0.5$$. Generate $$2000$$ realizations for $$(z_1, z_2)$$ from $$p(z_1,z_2)$$ and compute the corresponding $$(x_1, x_2)$$ pairs according to the given transformation. Then, maximize the log-likelihood of the $$(x_1, x_2)$$ observations to estimate the parameters of the forward mapping that transforms $$p(z)$$ into $$p(x)$$. Report the learned parameters, log-likelihood loss and a visualization of $$p(x)$$ at every step of your gradient ascent algorithm.
