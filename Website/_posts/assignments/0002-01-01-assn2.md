---
layout: assignment
categories:
 - assignment
title: "Assignment 2:  Statistical estimation, linear & logistic regression (due on 02/18)"
due:
tags:
 - week-4
permalink: /assn2/
---

## Problem 1 [20 Points]:

Suppose data $$\{x_i\}_{i = 1}^{i = n}$$ is drawn from a exponential distribution with parameter $$\lambda$$.
- Derive the MLE estimate $$\lambda_{\text{MLE}}$$.
- Derive the posterior distribution assuming the prior $$\lambda \sim \mathrm{Gam(\tau, \omega)}$$.
- Derive the MAP estimate $$\lambda_{\text{MAP}}$$.
- Explain how this MAP estimate differs from the MLE estimate, and provide an interpretation of $$\tau$$ and $$\omega$$ as to how they affect the estimate.

## Problem 2 [30 Points]:

Consider a Bayesian linear regression model where the outputs $$y$$ are distributed according to a Gaussian likelihood $$p(y\lvert x, \alpha, \beta, \gamma)$$ corresponding to a linear model $$y = \alpha x + \beta + \epsilon$$. Here $$\gamma$$ represents the noise level in the observed data, i.e. it corresponds to the precision of the data likelihood. In this case, the likelihood can be further expressed as:

$$p(y\lvert x, \alpha, \beta, \gamma) = (\sqrt{\frac{\gamma}{2\pi}})^n\exp(-\frac{\gamma(\sum_{i=1}^n\|y_i - x_i\alpha - \beta\|^2_2)}{2})$$

In a Bayesian setting we would like to assume prior distributions on the unknown parameters $$\alpha$$, $$\beta$$ and $$\gamma$$. Here we assume $$p(\alpha) = \mathcal{N}(0, 1)$$, $$p(\beta) = \mathcal{N}(0, 1)$$ and $$p(\gamma) = \textrm{Gam(2, 1)}$$ is a Gamma distribution.

Please write down the posterior conditional distribution for each of those parameters, i.e.,

$$p(\alpha\lvert x, y, \beta, \gamma),$$

$$p(\beta\lvert x, y, \alpha, \gamma),$$

$$p(\gamma\lvert x, y, \alpha, \beta),$$

Comment on your results.
(**HINT**: Might be easier to work with the $\log$ of the posterior, and "complete the square" to identify the distribution)

### Problem 3 [15 Points]

Download the MNIST data-set from [here](http://yann.lecun.com/exdb/mnist/).

(1) Parse the data and keep the 1,000 images and their associated labels for each of the following labels: 0, 1, 2. Further, randomly permute the data, and for each digit split the dataset to select 80 percent of the data for training and 20 percent for testing. Each image should have $$28\times 28$$ pixels.

(2) Implement a logistic regression model and train it using gradient descent only on label 0 and label 1. The loss function that you should use is the binary cross entropy loss. Report the accuracy of your model on the test data and provide a visualization of the resulting confusion matrix.

(3) Perform logistic regression on three labels (i.e. 0, 1, 2). In this case you need to use a one-hot encoding of the output labels and the multi-class cross entropy loss. Report the accuracy of your model on the test data and provide a visualization of the resulting confusion matrix.

### Problem 4 [35 Points]

Bayesian linear regression can be naturally extended to support approximation of nonlinear functions via the use of features or basis functions such as polynomials or trigonometric features. The case covered in class is just a special case of polynomial basis where you only use bias term and the identity mapping $$\phi(x) = x$$, $$\Phi(x) = [1, x]$$. It is straightforward to use other bases to do Bayesian linear regression. Here we consider three sets of 2D data both randomly sampled using $$N = 2500$$ observations within interval $$(x, y) \in[-1,1] \times [-1, 1]$$.

$$z_1(x, y) = \cos(10(x^2+y)) \sin(10(x+y^2)) + \epsilon,$$

$$z_2(x, y) = \frac{1}{(1 + 100 (x^2 - y^2)^2)} + \epsilon,$$

$$z_3(x, y) = \frac{1}{(1 + 100 (x^2 + y^2))} + \epsilon,$$

where $$\epsilon$$ corresponds to a zero mean uncorrelated Gaussian noise with standard deviation set equal to 5 percent of your data's standard deviation.

(1) Generate these three data sets and visualize them.
(2) Consider Bayesian linear regression models using a tensor product of 1D basis functions:
$$[\phi_1(x) \phi_1(y), \phi_1(x) \phi_2(y), \cdots \phi_1(x) \phi_m(y), \phi_2(x) \phi_1(y), \cdots \phi_m(x) \phi_m(y)]$$

Consider the following 1D basis functions:
- [Legendre basis](https://en.wikipedia.org/wiki/Legendre\_polynomials).

- Fourier basis: $$\Phi(x) = [1, \sin(\pi x), \cos(\pi x), \cdots, \sin(m \pi x), \cos(m \pi x)].$$

- RBF basis: $$\phi_1(x) = 1, \phi_i(x) = \exp(-100 (x - \xi_{i-1})^2) \forall i \neq 1, \xi = \texttt{linspace}(-1, 1, m).$$

Assuming the corresponding likelihood and priors:
$$p(z\lvert x, y, w) = \mathcal{N}(z \mid w^{T}\phi(x, y),\alpha^{-1}I), \ p(w)=\mathcal{N}(w\mid 0,\beta^{-1}I), \ \alpha=5, \beta = 0.1,$$

derive the expression of the objective of your MLE and MAP estimations.

(3) Implement JAX / Numpy code to do the regressions with these different basis for $$m = 15$$. Train your model and show the results for both data set including the training data, the MLE prediction, the MAP prediction and 5th and 95th percentiles from the 100 samples drawn from the predictive posterior distributions. Comments on your results.
