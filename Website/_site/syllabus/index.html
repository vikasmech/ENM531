<html>
  <!-- please find the included snippets in the /_includes directory -->
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link href="https://fonts.googleapis.com/css?family=Roboto:100,300,400,500|Playfair+Display:400,700" rel="stylesheet">

  <title>Data-driven Modeling and Probabilistic Scientific Computing</title>
  <meta name="description" content="">

  <link rel="canonical" href="https://www.seas.upenn.edu/~enm531/syllabus/">
  <link rel="alternate" type="application/rss+xml" title="Data-driven Modeling and Probabilistic Scientific Computing" href="https://www.seas.upenn.edu/~enm531/feed.xml">
  <link rel="shortcut icon" type ="image/png" href="https://www.seas.upenn.edu/~enm531/PredictiveIntelligence_logo.png">
  <!-- Styles -->
  <link rel="stylesheet" href="/~enm531/css/foundation.css">
  <!-- LaTeX support with MathJax -->
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
</head>


  <body>
    <div class="">
  <nav class="top-bar">
    <div class="top-bar-left">
      <ul class="menu">
	<li><a href="https://www.seas.upenn.edu/~enm531"><img src="https://www.seas.upenn.edu/~enm531/PredictiveIntelligence_logo.png" width="30" height="30"></a></li>
	<li><a href="https://github.com/PredictiveIntelligenceLab/ENM531"><img src="https://www.seas.upenn.edu/~enm531/GitHub_logo.png" width="30" height="30"></a></li>
        <li><a href="https://www.seas.upenn.edu/~enm531">Data-driven Modeling and Probabilistic Scientific Computing, Spring 2022</a></li>
        <li><a href="/~enm531/syllabus">Syllabus</a></li>
        <li><a href="/~enm531/duedates">Due Dates</a></li>
        <li><a href="/~enm531/policies">Policies</a></li>
      </ul>
    </div>
  </nav>
</div>

    
   <div class="row main">
      <!-- weekly plans including class plans, readings of the week and assignment of the week -->
<!-- tags are specifically reserved for grouping weekly content -->
<!-- see posts in assignments, classplans, and readings  -->


<section class="weekly">
  <h3>Week 1 - Introduction to data-driven modeling</h3>
  <p><ul>
  <li>Introduction</li>
  <li>Syllabus review</li>
  <li>Presentation of diverse applications that showcase the use of data, modeling, and scientific computation.</li>
  <li>Overview of the main themes and goals of this course</li>
  <li>Python tutorial</li>
</ul>
</p>
</section>
<section class="reading">
  
    
  
    
  
    
  
    
  
    
    <label>Reading</label>
    <ul>
  <li>The Emergence of Predictive Computational Science:
    <ul>
      <li>Computer predictions with quantified uncertainty <a class="citation" href="#oden2010computer">(Oden et al., 2010)</a></li>
      <li><a href="https://vimeo.com/102636357">Lecture</a> by J.T Oden.</li>
    </ul>
  </li>
  <li>Review papers on recent advances in machine learning:
    <ul>
      <li>Probabilistic machine learning and artificial intelligence <a class="citation" href="#ghahramani2015probabilistic">(Ghahramani, 2015)</a></li>
      <li>Deep learning <a class="citation" href="#lecun2015deep">(LeCun et al., 2015)</a></li>
      <li>Machine learning: Trends, perspectives, and prospects <a class="citation" href="#jordan2015machine">(Jordan &amp; Mitchell, 2015)</a></li>
    </ul>
  </li>
  <li>Scientific computing in Python:
    <ul>
      <li><a href="https://github.com/jrjohansson/scientific-python-lectures">Lectures and code</a> by Robert Johansson.</li>
    </ul>
  </li>
</ul>

<ol class="bibliography"><li><span id="oden2010computer">Oden, T., Moser, R., &amp; Ghattas, O. (2010). Computer predictions with quantified uncertainty, part I. <i>SIAM News</i>, <i>43</i>(9), 1–3.</span></li>
<li><span id="ghahramani2015probabilistic">Ghahramani, Z. (2015). Probabilistic machine learning and artificial intelligence. <i>Nature</i>, <i>521</i>(7553), 452–459.</span></li>
<li><span id="lecun2015deep">LeCun, Y., Bengio, Y., &amp; Hinton, G. (2015). Deep learning. <i>Nature</i>, <i>521</i>(7553), 436–444.</span></li>
<li><span id="jordan2015machine">Jordan, M. I., &amp; Mitchell, T. M. (2015). Machine learning: Trends, perspectives, and prospects. <i>Science</i>, <i>349</i>(6245), 255–260.</span></li></ol>

    
  
</section>
<section class="assignment">
  
    
  
    
      <label>Assignment</label>
      <a href="https://www.seas.upenn.edu/~enm531/assn0/">How to set up your computing environment</a>
    
  
</section>


<section class="weekly">
  <h3>Week 2 - Primer on probability and statistics</h3>
  <p><ul>
  <li>Basic rules of probability, Bayes theorem, discrete and continuous random variables, probability distributions and moments, transformations of random variables, correlation and independence, entropy and mutual information, statistical comparisons.</li>
</ul>
</p>
</section>
<section class="reading">
  
    
  
    
  
    
  
    
    <label>Reading</label>
    <ul>
  <li>Primer on probability and statistics:
    <ul>
      <li>Chapter 2 <a class="citation" href="#murphy2012machine">(Murphy, 2012)</a></li>
      <li>Section 1.2, Chapter 2 <a class="citation" href="#bishop2006pattern">(Bishop, 2006)</a></li>
    </ul>
  </li>
  <li>Useful properties of the multivariate Gaussian distribution:
    <ul>
      <li>Read the <a href="https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter13.pdf">notes</a> from Michael Jordan’s Bayesian Modeling and Inference class at Berkeley.</li>
    </ul>
  </li>
</ul>

<ol class="bibliography"><li><span id="murphy2012machine">Murphy, K. P. (2012). <i>Machine learning: a probabilistic perspective</i>. MIT press.</span></li>
<li><span id="bishop2006pattern">Bishop, C. M. (2006). <i>Pattern recognition and machine learning</i>. springer.</span></li></ol>

    
  
    
  
</section>
<section class="assignment">
  
    
      <label>Assignment</label>
      <a href="https://www.seas.upenn.edu/~enm531/assn1/">Assignment 1:  A primer on Probability and Statistics (due on 02/04)</a>
    
  
    
  
</section>


<section class="weekly">
  <h3>Week 3 - Statistical Estimation & Optimization</h3>
  <p><ul>
  <li>Maximum likelihood estimation</li>
  <li>Gradients and Hessians, gradient descent, Newton’s algorithm, stochastic gradient descent.</li>
  <li>Momentum acceleration and adaptive moment estimation</li>
</ul>
</p>
</section>
<section class="reading">
  
    
  
    
  
    
    <label>Reading</label>
    <ul>
  <li>Statistical estimation:
    <ul>
      <li>Chapter 2 <a class="citation" href="#bishop2006pattern">(Bishop, 2006)</a></li>
    </ul>
  </li>
  <li>Gradient descent:
    <ul>
      <li>Section 8.3.2 <a class="citation" href="#murphy2012machine">(Murphy, 2012)</a></li>
    </ul>
  </li>
  <li>Newton’s algorithm and L-BFGS:
    <ul>
      <li>Section 8.3.3,  8.3.5 <a class="citation" href="#murphy2012machine">(Murphy, 2012)</a></li>
    </ul>
  </li>
  <li>Stochastic gradient descent and its modern variants:
    <ul>
      <li>An overview of gradient descent optimization algorithms by Sebastian Ruder (see <a href="http://ruder.io/optimizing-gradient-descent/">blog post</a> and <a href="https://arxiv.org/pdf/1609.04747.pdf">article</a>)</li>
      <li><a href="https://distill.pub/2017/momentum/">Why momentum really works</a></li>
    </ul>
  </li>
</ul>

<ol class="bibliography"><li><span id="bishop2006pattern">Bishop, C. M. (2006). <i>Pattern recognition and machine learning</i>. springer.</span></li>
<li><span id="murphy2012machine">Murphy, K. P. (2012). <i>Machine learning: a probabilistic perspective</i>. MIT press.</span></li></ol>

    
  
    
  
    
  
</section>
<section class="assignment">
  
    
  
    
  
</section>


<section class="weekly">
  <h3>Week 4 - Linear regression</h3>
  <p><ul>
  <li>Linear regression</li>
  <li>Bayesian linear regression</li>
  <li>Linear regression with basis functions</li>
  <li>Logistic regression:</li>
  <li>Maximum likelihood estimation</li>
  <li>Newton’s algorithm and iterative re-weighted least squares</li>
</ul>
</p>
</section>
<section class="reading">
  
    
  
    
    <label>Reading</label>
    <ul>
  <li>Useful properties of the multivariate Gaussian distribution:
    <ul>
      <li>Read the <a href="https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter13.pdf">notes</a> from Michael Jordan’s Bayesian Modeling and Inference class at Berkeley.</li>
    </ul>
  </li>
  <li>Linear regression and Bayesian linear regression:
    <ul>
      <li>Chapter 3 <a class="citation" href="#bishop2006pattern">(Bishop, 2006)</a></li>
    </ul>
  </li>
  <li>Logistic regression:
    <ul>
      <li>Section 8.1 <a class="citation" href="#murphy2012machine">(Murphy, 2012)</a></li>
      <li>Section 4.3.2 - 4.3.4  <a class="citation" href="#bishop2006pattern">(Bishop, 2006)</a></li>
    </ul>
  </li>
</ul>

<ol class="bibliography"><li><span id="bishop2006pattern">Bishop, C. M. (2006). <i>Pattern recognition and machine learning</i>. springer.</span></li>
<li><span id="murphy2012machine">Murphy, K. P. (2012). <i>Machine learning: a probabilistic perspective</i>. MIT press.</span></li></ol>

    
  
    
  
    
  
    
  
</section>
<section class="assignment">
  
    
  
    
  
</section>


<!-- recommended reading for the semester -->
<hr>
<section class="general">
  
    
    <p>This is a list of great books to consult. I will give you selected chapters as part of the course readings.</p>

<ol class="bibliography"><li><span id="murphy2012machine">Murphy, K. P. (2012). <i>Machine learning: a probabilistic perspective</i>. MIT press.</span></li>
<li><span id="bishop2006pattern">Bishop, C. M. (2006). <i>Pattern recognition and machine learning</i>. springer.</span></li>
<li><span id="rasmussen2006gaussian">Rasmussen, C. E., &amp; Williams, C. K. I. (2006). <i>Gaussian processes for machine learning</i> (Vol. 1). MIT press Cambridge.</span></li>
<li><span id="mackay2003information">MacKay, D. J. C. (2003). <i>Information theory, inference and learning algorithms</i>. Cambridge university press.</span></li>
<li><span id="goodfellow2016deep">Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). <i>Deep learning</i>. MIT press.</span></li>
<li><span id="quarteroni2006approximation">Quarteroni, A., &amp; Saleri, F. (2006). <i>Approximation of functions and data</i>. Springer.</span></li>
<li><span id="kutz2013data">Kutz, J. N. (2013). <i>Data-driven modeling &amp; scientific computation: methods for complex systems &amp; big data</i>. Oxford University Press.</span></li>
<li><span id="iserles2009first">Iserles, A. (2009). <i>A first course in the numerical analysis of differential equations</i> (Number 44). Cambridge university press.</span></li>
<li><span id="strang1993introduction">Strang, G. (1993). <i>Introduction to linear algebra</i> (Vol. 3). Wellesley-Cambridge Press Wellesley, MA.</span></li>
<li><span id="golub2012matrix">Golub, G. H., &amp; Van Loan, C. F. (2012). <i>Matrix computations</i> (Vol. 3). JHU Press.</span></li>
<li><span id="boyd2004convex">Boyd, S., &amp; Vandenberghe, L. (2004). <i>Convex optimization</i>. Cambridge university press.</span></li></ol>

    
  
    
  
    
  
    
  
    
  
</section>

    </div>

        <script src="https://www.seas.upenn.edu/~enm531/js/vendor/jquery.js"></script>
    <script src="https://www.seas.upenn.edu/~enm531/js/vendor/foundation.min.js"></script>
    <script src="https://www.seas.upenn.edu/~enm531/js/vendor/what-input.js"></script>
    <script src="https://www.seas.upenn.edu/~enm531/js/app.js"></script>


    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-112328818-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-112328818-1');
</script>

    
  </body>

</html>
