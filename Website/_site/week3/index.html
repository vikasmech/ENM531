<ul>
  <li>Statistical estimation:
    <ul>
      <li>Chapter 2 <a class="citation" href="#bishop2006pattern">(Bishop, 2006)</a></li>
    </ul>
  </li>
  <li>Gradient descent:
    <ul>
      <li>Section 8.3.2 <a class="citation" href="#murphy2012machine">(Murphy, 2012)</a></li>
    </ul>
  </li>
  <li>Newtonâ€™s algorithm and L-BFGS:
    <ul>
      <li>Section 8.3.3,  8.3.5 <a class="citation" href="#murphy2012machine">(Murphy, 2012)</a></li>
    </ul>
  </li>
  <li>Stochastic gradient descent and its modern variants:
    <ul>
      <li>An overview of gradient descent optimization algorithms by Sebastian Ruder (see <a href="http://ruder.io/optimizing-gradient-descent/">blog post</a> and <a href="https://arxiv.org/pdf/1609.04747.pdf">article</a>)</li>
      <li><a href="https://distill.pub/2017/momentum/">Why momentum really works</a></li>
    </ul>
  </li>
</ul>

<ol class="bibliography"><li><span id="bishop2006pattern">Bishop, C. M. (2006). <i>Pattern recognition and machine learning</i>. springer.</span></li>
<li><span id="murphy2012machine">Murphy, K. P. (2012). <i>Machine learning: a probabilistic perspective</i>. MIT press.</span></li></ol>
